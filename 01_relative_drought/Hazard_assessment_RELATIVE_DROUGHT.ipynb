{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "114c11c0-af34-4523-92fb-9a0d62cd518b",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Hazard assessment for relative drought\n",
    "\n",
    "Click [![Binder](https://mybinder.org/badge_logo.svg)](https://mybinder.org/v2/gh/CLIMAAX/DROUGHTS/HEAD?labpath=01_relative_drought%2Hazard_assessment_RELATIVE_DROUGHT.ipynb) to launch this workflow on MyBinder. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f3b8a4d-966a-41f5-9e15-421fea4ae651",
   "metadata": {},
   "source": [
    "In this workflow drought hazard (dH) for a given region is estimated as the probability of exceedance the median of regional (e.g., EU level) severe precipitation deficits for an historical reference period (e.g. 1979-2019) or for a future projection period (e.g. 2015-2100). The methodology used here was developed and applied globally by Carrão et al. (2016) $^1$.\n",
    "\n",
    "Workflow on how to quantify drough risk as the product of drought hazard, exposure and vulnerability can be found in the Risk_assessement notebook. Visualization tools based on preprocessed results for both drought hazard and drought risk can be found in Risk_visualization notebook.\n",
    " \n",
    "Below is a description of the data and tools used to calculate droguth hazard, both for the historic period and for future scenarios, and the outputs of this workflow.\n",
    "\n",
    "![hazard.png](images/hazard.png)\n",
    "\n",
    "We use the weighted anomaly of standardized precipitation (WASP) index to define the severity of precipitation deficit. The WASP-index takes into account the annual seasonality of the precipitation cycle and is computed by summing weighted standardized monthly precipitation anomalies (see Eq. 1). Where $P_{n,m}$ is each region's monthly precipitation, $T_m$ is a monthly threshold defining precipitation severity, and $T_A$ is an annual threshold for precipitation severity. The thresholds are defined by dividing multi-annual monthly observed rain using the 'Fisher-jenks' classification algorithm $^2$. \n",
    "\n",
    "Eq. 1:\n",
    "\n",
    "$$WASP_j = \\Sigma_{P_{n,m} < T_m}^{P_{n,m} >= T_m}( \\frac{P_{n,m} - T_m}{T_m})*\\frac{T_m}{T_A}$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cf6c3e8",
   "metadata": {},
   "source": [
    "## Workflow implementation\n",
    "\n",
    "### Load libraries\n",
    "\n",
    ":::{admonition} Find more info about the libraries used in this workflow here\n",
    ":class: hint dropdown\n",
    "\n",
    "- [os](https://docs.python.org/3/library/os.html) - To create directories and work with files\n",
    "- [urllib](https://docs.python.org/3/library/urllib.html), [pooch](https://www.fatiando.org/pooch/latest/index.html) - To access and download online resources\n",
    "- [pandas](https://pandas.pydata.org/docs/user_guide/index.html) - To create and manage data frames (tables) in Python\n",
    "- [geopandas](https://geopandas.org/en/stable/docs.html) - Extend pandas to store and manipulate spatial data\n",
    "- [numpy](https://numpy.org/doc/stable/) - For basic math tools and operations\n",
    "- [scipy](https://scipy.org/) - Provide advanced mathematical tools and optimization capacities \n",
    "- [jenkspy](https://github.com/mthh/jenkspy) - To apply Fisher-Jenks alogrithm \n",
    "- [json](https://docs.python.org/3/library/json.html) - To load, store and manipuilate JSON objects\n",
    "- [pyproj](https://pyproj4.github.io/pyproj/stable/) - An interface to a geographic projections and transformations library\n",
    "- [matplotlib](https://matplotlib.org/) - For plotting\n",
    "- [plotly](https://plotly.com/python/) - For dynamic and interactive plotting\n",
    "- [datetime](https://docs.python.org/3/library/datetime.html) - For handling dates in Python\n",
    ":::"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d06a2286",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import urllib\n",
    "import pooch\n",
    "os.environ['USE_PYGEOS'] = '0'\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import numpy as np\n",
    "import scipy\n",
    "import jenkspy\n",
    "import json\n",
    "import pyproj\n",
    "import plotly.express as px\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime\n",
    "from datetime import date"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b5b9ec1",
   "metadata": {},
   "source": [
    "### Define working environment and global parameters\n",
    "\n",
    "This workflow relies on pre-processed data. The user will define the path to the data folder and the code below would create a folder for outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2ad5f46f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set working environment\n",
    "workflow_folder = './sample_data_nuts3/'\n",
    "\n",
    "# Define scenario 0: historic; 1: SSP1-2.6; 2: SSP3-7.0. 3: SSP5-8.5\n",
    "scn = 0\n",
    "\n",
    "# Define time (applicible only for the future): 0: near-future (2050); 1: far-future (2080)\n",
    "time = 0\n",
    "\n",
    "pattern = \"historic\"\n",
    "pattern_h = \"historic\"\n",
    "if scn != 0:\n",
    "    pattern_h = ['ssp126', 'ssp370', 'ssp585'][scn - 1]\n",
    "    pattern = ['ssp126', 'ssp370', 'ssp585'][scn - 1] + '_' + ['nf', 'ff'][time]\n",
    "\n",
    "# debug if folder does not exist - issue an error to check path\n",
    "\n",
    "# Create outputs folder\n",
    "name_output_folder = 'outputs_hazards'\n",
    "os.makedirs(os.path.join(workflow_folder, name_output_folder), exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0481754f-dcaa-48e1-925e-a7a78752903b",
   "metadata": {},
   "source": [
    "### Access to sample dataset\n",
    "\n",
    "Load the file registry for the `droughtrisk_sample_nuts3` dataset in the CLIMAAX cloud storage with pooch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "65de35cd-098c-480d-b829-3757fbc5dc06",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_data_pooch = pooch.create(\n",
    "    path=workflow_folder,\n",
    "    base_url=\"https://object-store.os-api.cci1.ecmwf.int/climaax/droughtrisk_sample_nuts3/\"\n",
    ")\n",
    "sample_data_pooch.load_registry(\"files_registry.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cccc203b",
   "metadata": {},
   "source": [
    "If any files requested below were downloaded before, pooch will inspect the local file contents and skip the download if the contents match expectations.\n",
    "\n",
    "### Choose country code\n",
    "\n",
    "Choose country code from:  \n",
    "'HR', 'DE', 'BG', 'AT', 'AL', 'BE', 'ES', 'CH', 'CZ', 'EL', 'FR', 'FI', 'EE', 'DK', 'CY', 'HU', 'NL', 'NO', 'LV', 'LT', 'IS', 'MK', 'MT', 'IT', 'TR', 'PL', 'RO', 'SE', 'RS', 'PT', 'IE', 'UK', 'ME', 'LU', 'SK', 'SI' ,'LI'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4e5b6ff7",
   "metadata": {},
   "outputs": [],
   "source": [
    "ccode = \"AL\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50eb7d4d",
   "metadata": {},
   "source": [
    "### Load and visualize precipitation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "636f561d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analyzing drought hazard. This process may take few minutes...\n",
      "\n",
      "\n",
      "The following regions are dropped due to missing data: []\n",
      "\n",
      "\n",
      "Input precipitation data (top 3 rows): \n",
      "      timing     AL011     AL012     AL013    AL014     AL015     AL021  \\\n",
      "0 2020-01-31  0.000220  0.000073  0.000197  0.00013  0.000377  0.000199   \n",
      "1 2020-02-29  0.000279  0.000100  0.000245  0.00016  0.000459  0.000255   \n",
      "2 2020-03-31  0.000258  0.000082  0.000235  0.00015  0.000418  0.000224   \n",
      "\n",
      "      AL022     AL031     AL032     AL033     AL034     AL035  \n",
      "0  0.000187  0.000174  0.000174  0.000278  0.000230  0.000245  \n",
      "1  0.000242  0.000231  0.000218  0.000394  0.000300  0.000343  \n",
      "2  0.000208  0.000188  0.000179  0.000299  0.000249  0.000249  \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Download precipitation data for selected scenario\n",
    "precip_file = sample_data_pooch.fetch(f\"drought_hazard_{pattern_h}.csv\")\n",
    "\n",
    "print(\"Analyzing drought hazard. This process may take few minutes...\")\n",
    "print('\\n')\n",
    "\n",
    "# Load precipitation data\n",
    "precip = pd.read_csv(precip_file)\n",
    "# Convert timing column to datetime\n",
    "precip['timing'] = pd.to_datetime(precip['timing'], format='%Y-%m-%d')\n",
    "#'%b-%Y'\n",
    "\n",
    "# time  subset\n",
    "\n",
    "if scn != 0:\n",
    "    if time == 0:\n",
    "        precip = precip.loc[(precip['timing'].dt.date >= date(2020,1,1)) & (precip['timing'].dt.date  < date(2060,1,1)), :]\n",
    "    else:\n",
    "        precip = precip.loc[(precip['timing'].dt.date >= date(2060,1,1)) & (precip['timing'].dt.date  < date(2100,1,1)), :]\n",
    "else:\n",
    "    precip = precip.loc[(precip['timing'].dt.date >= date(1979,1,1)) & (precip['timing'].dt.date  < date(2020,1,1)), :]\n",
    "\n",
    "# col_subset aims to extract the relevant results\n",
    "precip = precip.reset_index()\n",
    "\n",
    "col_subset = list(precip.columns.str.contains(ccode))\n",
    "col_subset[1] = True\n",
    "precip = precip.loc[:, col_subset]\n",
    "\n",
    "# clean NaN rows & missing columns\n",
    "precip = precip.loc[~np.array(precip.isna().all(axis = 1)),:]\n",
    "\n",
    "drop_regions = []\n",
    "\n",
    "# missing data in columns\n",
    "col_subset = np.array(precip.isna().all(axis = 0))\n",
    "drop_regions += list(precip.columns[col_subset])\n",
    "precip = precip.loc[:, ~col_subset]\n",
    "\n",
    "regions = precip.columns[1:]\n",
    "output = pd.DataFrame(regions, columns = ['NUTS_ID'])\n",
    "\n",
    "print(\"The following regions are dropped due to missing data: \"+ str(drop_regions))\n",
    "print('\\n')\n",
    "print('Input precipitation data (top 3 rows): ')\n",
    "print(precip.head(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fd605c8",
   "metadata": {},
   "source": [
    "### Calculate WASP Index (Weighted Anomaly Standardized Precipitation) monthly threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "27ccf421",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create empty arrays and tables for intermediate and final results\n",
    "WASP = []\n",
    "WASP_global = []\n",
    "drought_class = precip.copy()\n",
    "\n",
    "# prepare output for drought event index - WASP_j- list of lists wasp = [[rid1], [rid2], ...]\n",
    "for i in range(1, len(precip.columns)):\n",
    "\n",
    "\n",
    "    # For every NUTS3 out of all regions - do the following:\n",
    "\n",
    "    # empty array for the monthly water deficit thresholds\n",
    "    if scn == 0:\n",
    "        t_m = pd.DataFrame(np.tile([0], (12, len(precip.columns) - 1)))\n",
    "    else:\n",
    "        t_m_file = download_sample_data(os.path.join(name_output_folder, f'tm_hist_{ccode}.csv'))\n",
    "        t_a_file = download_sample_data(os.path.join(name_output_folder, f'ta_hist_{ccode}.csv'))\n",
    "        t_m = pd.read_csv(t_m_file)\n",
    "        t_a = list(pd.read_csv(t_a_file).t_a)\n",
    "\n",
    "    for mon_ in range(1, 13):\n",
    "        # For every month out of all all months (January, ..., December) - do the following:\n",
    "\n",
    "        # calculate monthly drought threshold -\\\n",
    "            # using a division of the data into to clusters with the Jenks' (Natural breaks) algorithm\n",
    "        r_idx = precip.index[precip.timing.dt.month == mon_].tolist()\n",
    "        if pattern == 'historic':\n",
    "            t_m_last = jenkspy.jenks_breaks(precip.iloc[r_idx, i], n_classes = 2)[1]\n",
    "            t_m.iloc[mon_ - 1, i - 1] = t_m_last\n",
    "\n",
    "\n",
    "        # Define every month with water deficity (precipitation < threshold) as a drought month\n",
    "        drought_class.iloc[r_idx, i] = (drought_class.iloc[r_idx, i] < t_m.iloc[mon_ - 1, i - 1]).astype(int)\n",
    "\n",
    "    # calculate annual water deficit threshold\n",
    "    if pattern == 'historic':\n",
    "        t_a = list(t_m.sum(axis = 0))\n",
    "\n",
    "    t_m0 = t_m.iloc[:, i - 1]\n",
    "    t_a0 = t_a[i-1]\n",
    "    # calculate droughts' magnitude and duration using the WASP indicator\n",
    "    WASP_tmp = []\n",
    "    first_true=0\n",
    "    index = []\n",
    "    for k in range(1, len(precip)):\n",
    "        # for every row (ordered month-year combinations):\n",
    "            # check if drought month -> calculate drought accumulated magnitude (over 1+ months)\n",
    "        if drought_class.iloc[k, i]== 1:\n",
    "            # In case of a drought month\n",
    "            # calculate monthly WASP index\n",
    "            index = int(drought_class.timing.dt.month[k] - 1)\n",
    "            # WASP monthly index: [(precipitation - month_threshold)/month_threshold)]*[month_threshold/annual_treshold]\n",
    "            WASP_last=((precip.iloc[k,i] - t_m0[index])/t_m0[index])* (t_m0[index]/t_a0)\n",
    "\n",
    "            if first_true==0:\n",
    "                # if this is the first month in a drought event:\n",
    "                # append calculated monthly wasp to WASP array.\n",
    "                WASP_tmp.append(WASP_last)\n",
    "                first_true=1\n",
    "            else:\n",
    "                # if this is NOT the first month in a drought event:\n",
    "                # add the calculated monthly wasp to last element in the WASP array (accumulative drought).\n",
    "                WASP_tmp[-1]=WASP_tmp[-1] + WASP_last\n",
    "            WASP_global.append(WASP_last)\n",
    "        else:\n",
    "            # check if not drought month - do not calculate WASP\n",
    "            first_true=0\n",
    "    WASP.append(np.array(WASP_tmp))\n",
    "\n",
    "if pattern == 'historic':\n",
    "    t_m.to_csv(os.path.join(workflow_folder, name_output_folder, f'tm_hist_{ccode}.csv'),\\\n",
    "              index = False)\n",
    "    pd.DataFrame({'t_a': t_a}).to_csv(os.path.join(workflow_folder, name_output_folder, f'ta_hist_{ccode}.csv'),\\\n",
    "                                    index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ad44b5d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>>>> Drought hazard is completed.\n",
      ">>>>> Drought hazard indices were saved.\n"
     ]
    }
   ],
   "source": [
    "dH = []\n",
    "WASP = np.array(WASP, dtype=object)\n",
    "# calculate global median deficit severity -\n",
    "    # set drought hazard (dH) as the probability of exceeding the global median water deficit.\n",
    "\n",
    "median_global_wasp = np.nanmedian(WASP_global)\n",
    "\n",
    "# calculate dH per region i\n",
    "for i in range(WASP.shape[0]):\n",
    "    # The more negative the WASP index, the more severe is the deficit event, so\n",
    "    # probability of exceedence the severity is 1 - np.nansum(WASP[i] >= median_global_wasp) / len(WASP[i])\n",
    "    if len(WASP[i]) > 0:\n",
    "        dH.append(round(1 - np.nansum(WASP[i] >= median_global_wasp) / len(WASP[i]), 3))\n",
    "    else:\n",
    "        dH.append(0.)\n",
    "\n",
    "\n",
    "# https://agupubs.onlinelibrary.wiley.com/doi/pdf/10.1029/2004GL020901 - WASP Indicator description\n",
    "\n",
    "output['wasp_raw_mean'] = [np.nanmean(x) for x in WASP]\n",
    "output['wasp_raw_q25'] = [np.nanquantile(x, q = 0.25) for x in WASP]\n",
    "output['wasp_raw_median'] = [np.nanmedian(x) for x in WASP]\n",
    "output['wasp_raw_q75'] = [np.nanquantile(x, q = 0.75) for x in WASP]\n",
    "output['wasp_raw_count'] = [x.shape[0] for x in WASP]\n",
    "\n",
    "output['hazard_raw'] = dH\n",
    "print('>>>>> Drought hazard is completed.')\n",
    "\n",
    "output.to_csv(os.path.join(workflow_folder, name_output_folder, f'droughthazard_{ccode}_{pattern}.csv'),\\\n",
    "              index = False)\n",
    "print('>>>>> Drought hazard indices were saved.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bd6a645",
   "metadata": {},
   "source": [
    "## Conclusions\n",
    "\n",
    "The above workflow calculates the Drought hazard (dH) index that can be used as an input to calculate drought risk in the workflow described in the file Risk_Assessment.ipynb."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58a12200-0053-4e52-945e-844b6ad72543",
   "metadata": {},
   "source": [
    "## Contributors\n",
    "\n",
    "The workflow has beend developed by [Silvia Artuso](https://iiasa.ac.at/staff/silvia-artuso) and [Dor Fridman](https://iiasa.ac.at/staff/dor-fridman) from [IIASA's Water Security Research Group](https://iiasa.ac.at/programs/biodiversity-and-natural-resources-bnr/water-security), and supported by [Michaela Bachmann](https://iiasa.ac.at/staff/michaela-bachmann) from [IIASA's Systemic Risk and Reslience Research Group](https://iiasa.ac.at/programs/advancing-systems-analysis-asa/systemic-risk-and-resilience)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c6adf00",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "[1] Carrão, H., Naumann, G., & Barbosa, P. (2016). Mapping global patterns of drought risk: An empirical framework based on sub-national estimates of hazard, exposure and vulnerability. *Global Environmental Change*, 39, 108-124."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
